# -*- coding: utf-8 -*-
"""quora_duplicate_ques.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xTAP6bnWL9XmKhTcxca2sXuavXFAOfYr
"""

from google.colab import drive

drive.mount('/content/drive')

import os
os.chdir("/content/drive/MyDrive/duplicate_ques_train_dataset")
print("Files in dataset folder:", os.listdir())

import pandas as pd
new_df = pd.read_csv("duplicate_ques_trainn.csv")

df = new_df.sample(30000,random_state=2)

df.head(10)

df.shape

df.info()

df.isnull().sum()

print(df.duplicated().sum())

print(df['is_duplicate'].value_counts())
print((df['is_duplicate'].value_counts()/df.shape[0])*100)
df['is_duplicate'].value_counts().plot(kind='bar')

import numpy as np

# Repeated questions

qid = pd.Series(df['qid1'].tolist() + df['qid2'].tolist())
print('Number of unique questions', np.unique(qid).shape[0])

x = qid.value_counts()>1
print('Number of questions getting repeated', x[x].shape[0])

import matplotlib.pyplot as plt

# Repeated questions histogram

plt.hist(qid.value_counts().values, bins=160)
plt.yscale('log')
plt.xlabel('Number of times a question is repeated')
plt.ylabel('Number of questions')
plt.show()

# Test Preprocessing
from bs4 import BeautifulSoup
import re

def preprocess(q):
  q = str(q).lower().strip()


  # Replace certain special characters with their string equivalents
  q = q.replace('%','percent')
  q = q.replace('$','dollar')
  q = q.replace('@','at')
  q = q.replace('₹','rupee')
  q = q.replace('€','euro')

  q = q.replace('[math]', '')             # math word appears more frequently

  # Replacing some numbers with string equivalents (not perfect, can be done better to account for more cases)
  q = q.replace(',000,000,000 ', 'b ')
  q = q.replace(',000,000 ', 'm ')
  q = q.replace(',000 ', 'k ')
  q = re.sub(r'([0-9]+)000000000', r'\1b', q)
  q = re.sub(r'([0-9]+)000000', r'\1m', q)
  q = re.sub(r'([0-9]+)000', r'\1k', q)

  # Decontracting words

  contractions = {
    "ain't": "am not",
    "aren't": "are not",
    "can't": "can not",
    "can't've": "can not have",
    "'cause": "because",
    "could've": "could have",
    "couldn't": "could not",
    "couldn't've": "could not have",
    "didn't": "did not",
    "doesn't": "does not",
    "don't": "do not",
    "hadn't": "had not",
    "hadn't've": "had not have",
    "hasn't": "has not",
    "haven't": "have not",
    "he'd": "he would",
    "he'd've": "he would have",
    "he'll": "he will",
    "he'll've": "he will have",
    "he's": "he is",
    "how'd": "how did",
    "how'd'y": "how do you",
    "how'll": "how will",
    "how's": "how is",
    "i'd": "i would",
    "i'd've": "i would have",
    "i'll": "i will",
    "i'll've": "i will have",
    "i'm": "i am",
    "i've": "i have",
    "isn't": "is not",
    "it'd": "it would",
    "it'd've": "it would have",
    "it'll": "it will",
    "it'll've": "it will have",
    "it's": "it is",
    "let's": "let us",
    "ma'am": "madam",
    "mayn't": "may not",
    "might've": "might have",
    "mightn't": "might not",
    "mightn't've": "might not have",
    "must've": "must have",
    "mustn't": "must not",
    "mustn't've": "must not have",
    "needn't": "need not",
    "needn't've": "need not have",
    "o'clock": "of the clock",
    "oughtn't": "ought not",
    "oughtn't've": "ought not have",
    "shan't": "shall not",
    "sha'n't": "shall not",
    "shan't've": "shall not have",
    "she'd": "she would",
    "she'd've": "she would have",
    "she'll": "she will",
    "she'll've": "she will have",
    "she's": "she is",
    "should've": "should have",
    "shouldn't": "should not",
    "shouldn't've": "should not have",
    "so've": "so have",
    "so's": "so as",
    "that'd": "that would",
    "that'd've": "that would have",
    "that's": "that is",
    "there'd": "there would",
    "there'd've": "there would have",
    "there's": "there is",
    "they'd": "they would",
    "they'd've": "they would have",
    "they'll": "they will",
    "they'll've": "they will have",
    "they're": "they are",
    "they've": "they have",
    "to've": "to have",
    "wasn't": "was not",
    "we'd": "we would",
    "we'd've": "we would have",
    "we'll": "we will",
    "we'll've": "we will have",
    "we're": "we are",
    "we've": "we have",
    "weren't": "were not",
    "what'll": "what will",
    "what'll've": "what will have",
    "what're": "what are",
    "what's": "what is",
    "what've": "what have",
    "when's": "when is",
    "when've": "when have",
    "where'd": "where did",
    "where's": "where is",
    "where've": "where have",
    "who'll": "who will",
    "who'll've": "who will have",
    "who's": "who is",
    "who've": "who have",
    "why's": "why is",
    "why've": "why have",
    "will've": "will have",
    "won't": "will not",
    "won't've": "will not have",
    "would've": "would have",
    "wouldn't": "would not",
    "wouldn't've": "would not have",
    "y'all": "you all",
    "y'all'd": "you all would",
    "y'all'd've": "you all would have",
    "y'all're": "you all are",
    "y'all've": "you all have",
    "you'd": "you would",
    "you'd've": "you would have",
    "you'll": "you will",
    "you'll've": "you will have",
    "you're": "you are",
    "you've": "you have"
    }

  q_decontracted = []

  for word in q.split():
    if word in contractions:
      word = contractions[word]
    q_decontracted.append(word)
  q = ' '.join(q_decontracted)
  q = q.replace("'ve", " have")
  q = q.replace("n't", " not")
  q = q.replace("'re", " are")
  q = q.replace("'ll", " will")
  # Removing HTML tags
  q = BeautifulSoup(q)
  q = q.get_text()

  # Remove punctuations
  pattern = re.compile('\W')
  q = re.sub(pattern, ' ', q).strip()

  return q

# print(df['question1'].apply(lambda doc: " ".join(str(doc).split())).str.count(r'\bmath\b', flags=0).sum())

preprocess("I've already! wasn't <b>done<b/>?")

df['question1'] = df['question1'].apply(preprocess)
df['question2'] = df['question2'].apply(preprocess)

df.head()

# Removing stopword
import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords
stopwords.words('english')

# stop_words = set(stopwords.words('english'))

# def remove_stopword(text):
#   return " ".join([word for word in text.split() if word not in stop_words])

# df['question1'] = df['question1'].apply(remove_stopword)
# df['question2'] = df['question2'].apply(remove_stopword)

df.head()

# stemming

from nltk.stem.porter import PorterStemmer
ps = PorterStemmer()

def stem_word(text):
  return " ".join([ps.stem(word) for word in text.split()])

df['question1'] = df['question1'].apply(stem_word)
df['question2'] = df['question2'].apply(stem_word)

df.head()

df['q1_len'] = df['question1'].str.len()
df['q2_len'] = df['question2'].str.len()

df['q1_num_words'] = df['question1'].apply(lambda row: len(row.split(" ")))
df['q2_num_words'] = df['question2'].apply(lambda row: len(row.split(" ")))

df.head()

def common_words(row):
  w1 = set(map(lambda word:word.lower().strip(), row['question1'].split(" ")))
  w2 = set(map(lambda word:word.lower().strip(), row['question2'].split(" ")))
  return len(w1 & w2)

df['word_common'] = df.apply(common_words, axis=1)

df.head()

def total_words(row):
  w1 = set(map(lambda word:word.lower().strip(), row['question1'].split(" ")))
  w2 = set(map(lambda word:word.lower().strip(), row['question2'].split(" ")))
  return len(w1) + len(w2)

df['word_total'] = df.apply(total_words, axis=1)

df.head()

df['word_share'] = round(df['word_common']/df['word_total'], 2)

df.head()

#Advance feature creation

def fetch_token_features(row):

    q1 = row['question1']
    q2 = row['question2']

    SAFE_DIV = 0.0001

    STOP_WORDS = stopwords.words("english")

    token_features = [0.0]*8

    # Converting the Sentence into Tokens:
    q1_tokens = q1.split()
    q2_tokens = q2.split()

    if len(q1_tokens) == 0 or len(q2_tokens) == 0:
        return token_features

    # Get the non-stopwords in Questions
    q1_words = set([word for word in q1_tokens if word not in STOP_WORDS])
    q2_words = set([word for word in q2_tokens if word not in STOP_WORDS])

    #Get the stopwords in Questions
    q1_stops = set([word for word in q1_tokens if word in STOP_WORDS])
    q2_stops = set([word for word in q2_tokens if word in STOP_WORDS])

    # Get the common non-stopwords from Question pair
    common_word_count = len(q1_words.intersection(q2_words))

    # Get the common stopwords from Question pair
    common_stop_count = len(q1_stops.intersection(q2_stops))

    # Get the common Tokens from Question pair
    common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))

    token_features[0] = common_word_count / (min(len(q1_words), len(q2_words)) + SAFE_DIV)
    token_features[1] = common_word_count / (max(len(q1_words), len(q2_words)) + SAFE_DIV)
    token_features[2] = common_stop_count / (min(len(q1_stops), len(q2_stops)) + SAFE_DIV)
    token_features[3] = common_stop_count / (max(len(q1_stops), len(q2_stops)) + SAFE_DIV)
    token_features[4] = common_token_count / (min(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)
    token_features[5] = common_token_count / (max(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)

    # Last word of both question is same or not
    token_features[6] = int(q1_tokens[-1] == q2_tokens[-1])

    # First word of both question is same or not
    token_features[7] = int(q1_tokens[0] == q2_tokens[0])

    return token_features

token_features = df.apply(fetch_token_features, axis=1)

df['cwc_min']      = list(map(lambda x:x[0], token_features))
df['cwc_max']      = list(map(lambda x:x[1], token_features))
df['csc_min']      = list(map(lambda x:x[2], token_features))
df['csc_max']      = list(map(lambda x:x[3], token_features))
df['ctc_min']      = list(map(lambda x:x[4], token_features))
df['ctc_max']      = list(map(lambda x:x[5], token_features))
df['last_word_eq'] = list(map(lambda x:x[6], token_features))
df['first_word_eq'] = list(map(lambda x:x[7], token_features))

df.head()

!pip install distance

import distance

def fetch_length_features(row):

  q1 = row['question1']
  q2 = row['question2']

  length_features = [0.0]*3

  q1_tokens = q1.split()
  q2_tokens = q2.split()

  if len(q1_tokens) == 0 or len(q2_tokens) == 0:
    return length_features

  #absolute length feature
  length_features[0] = abs(len(q1_tokens) - len(q2_tokens))

  #avg token length of both questions
  length_features[1] = (len(q1_tokens) + len(q2_tokens))/2

  #longest_substr_ratio
  strs = list(distance.lcsubstrings(q1,q1))
  length_features[2] = len(strs[0]) / (min(len(q1), len(q2)) + 1)

  return length_features

length_features = df.apply(fetch_length_features, axis=1)

df['abs_len_diff'] = list(map(lambda x:x[0], length_features))
df['mean_len'] = list(map(lambda x:x[1], length_features))
df['longest_substr_ratio'] = list(map(lambda x:x[2], length_features))

df.head()

!pip install fuzzywuzzy

!pip install rapidfuzz

# Fuzzy Features
from rapidfuzz import fuzz

def fetch_fuzzy_features(row):

    q1 = row['question1']
    q2 = row['question2']

    #Convert anything non-string to a safe string
    if not isinstance(q1, str):
        q1 = "" if pd.isna(q1) else str(q1)
    if not isinstance(q2, str):
        q2 = "" if pd.isna(q2) else str(q2)

    fuzzy_features = [0.0]*4

    # fuzz_ratio
    fuzzy_features[0] = fuzz.QRatio(q1, q2)

    # fuzz_partial_ratio
    fuzzy_features[1] = fuzz.partial_ratio(q1, q2)

    # token_sort_ratio
    fuzzy_features[2] = fuzz.token_sort_ratio(q1, q2)

    # token_set_ratio
    fuzzy_features[3] = fuzz.token_set_ratio(q1, q2)

    return fuzzy_features

# force entire columns to string (extra safety)
df['question1'] = df['question1'].astype(str)
df['question2'] = df['question2'].astype(str)

fuzzy_features = df.apply(fetch_fuzzy_features, axis=1)

# Creating new feature columns for fuzzy features
df['fuzz_ratio'] = list(map(lambda x: x[0], fuzzy_features))
df['fuzz_partial_ratio'] = list(map(lambda x: x[1], fuzzy_features))
df['token_sort_ratio'] = list(map(lambda x: x[2], fuzzy_features))
df['token_set_ratio'] = list(map(lambda x: x[3], fuzzy_features))

df.shape

df.head()

import matplotlib.pyplot as plt
import seaborn as sns

sns.pairplot(df[['ctc_min', 'cwc_min', 'csc_min', 'is_duplicate']], hue='is_duplicate')

sns.pairplot(df[['ctc_max', 'cwc_max', 'csc_max', 'is_duplicate']],hue='is_duplicate')

sns.pairplot(df[['last_word_eq','first_word_eq','is_duplicate']],hue='is_duplicate')

sns.pairplot(df[['mean_len', 'abs_len_diff','longest_substr_ratio', 'is_duplicate']],hue='is_duplicate')

sns.pairplot(df[['fuzz_ratio', 'fuzz_partial_ratio','token_sort_ratio','token_set_ratio', 'is_duplicate']],hue='is_duplicate')

# Using TSNE for dimentionality reduction for 15 features(generated after cleaning the data) to 3 dimention

from sklearn.preprocessing import MinMaxScaler

X = MinMaxScaler().fit_transform(df[['cwc_min', 'cwc_max', 'csc_min', 'csc_max' , 'ctc_min' , 'ctc_max' , 'last_word_eq', 'first_word_eq' , 'abs_len_diff' , 'mean_len' , 'token_set_ratio' , 'token_sort_ratio' ,  'fuzz_ratio' , 'fuzz_partial_ratio' , 'longest_substr_ratio']])
y = df['is_duplicate'].values

# from sklearn.manifold import TSNE

# tsne2d = TSNE(
#     n_components=2,
#     init='random',    #pca
#     random_state=101,
#     method='barnes_hut',
#     n_iter=1000,
#     verbose=2,
#     angle=0.5
# ).fit_transform(X)

# x_df = pd.DataFrame({'x':tsne2d[:,0], 'y':tsne2d[:,1] ,'label':y})

# # draw the plot in appropriate place in the grid
# sns.lmplot(data=x_df, x='x', y='y', hue='label', fit_reg=False, height=6, aspect=1.2, palette="Set1",markers=['s','o'])

# tsne3d = TSNE(
#     n_components=3,
#     init='random',
#     random_state=101,
#     method='barnes_hut',
#     n_iter=1000,
#     verbose=2,
#     angle=0.5
# ).fit_transform(X)

# import plotly.graph_objs as go
# from plotly.offline import init_notebook_mode

# init_notebook_mode(connected=True)

# trace1 = go.Scatter3d(
#     x=tsne3d[:,0],
#     y=tsne3d[:,1],
#     z=tsne3d[:,2],
#     mode='markers',
#     marker=dict(
#         sizemode='diameter',
#         color=y,
#         colorscale='Portland',
#         colorbar=dict(title='duplicate'),
#         line=dict(color='rgb(255,255,255)'),
#         opacity=0.75
#     )
# )

# fig = go.Figure(data=[trace1])
# fig.update_layout(
#     height=800,
#     width=800,
#     title='3D embedding with engineered features'
# )

# fig.show()

ques_df = df[['question1','question2']]
ques_df.head(5)

final_df = df.drop(columns=['id','qid1','qid2','question1','question2'])
print(final_df.shape)
final_df.head()

!pip install gensim

import gensim
from gensim.models import Word2Vec
import numpy as np

#Tokenize all questions
questions = list(ques_df['question1'] + ques_df['question2'])
questions_tokens  = [q.lower().split() for q in questions]

# Train Word2Vec on all questions
w2v_model = Word2Vec(
    questions_tokens,
    vector_size=300,
    window=5,
    min_count=1,
    workers=4,
    sg=1
)

# sentence embedding by averaging word vectors
def sentence_vector(tokens,model):
  vecs = [model.wv[word] for word in tokens if word in model.wv]
  if not vecs:
    return np.zeros(model.vector_size)
  return np.mean(vecs, axis=0)

# Generate embeddings for question1 and question2
q1_arr = np.array([sentence_vector(q.lower().split(), w2v_model) for q in ques_df['question1']])
q2_arr = np.array([sentence_vector(q.lower().split(), w2v_model) for q in ques_df['question2']])

print(q1_arr.shape, q2_arr.shape)

temp_df1 = pd.DataFrame(q1_arr, index= ques_df.index)
temp_df2 = pd.DataFrame(q2_arr, index= ques_df.index)
temp_df = pd.concat([temp_df1, temp_df2], axis=1)
temp_df.shape

final_df = pd.concat([final_df,temp_df],axis=1)

final_df.shape

final_df.head()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    final_df.iloc[:,1:].values,
    final_df.iloc[:,0].values,
    test_size=0.2,
    random_state=1
)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
rf = RandomForestClassifier()
rf.fit(X_train,y_train)
y_pred = rf.predict(X_test)
accuracy_score(y_test,y_pred)

from xgboost import XGBClassifier
xgb = XGBClassifier()
xgb.fit(X_train,y_train)
y_pred1 = xgb.predict(X_test)
accuracy_score(y_test,y_pred1)

from sklearn.metrics import confusion_matrix

confusion_matrix(y_test,y_pred)

confusion_matrix(y_test,y_pred1)

# conclusion Random forest is best(confusion matrix is the answer)

#Testing

def test_common_words(q1,q2):
    w1 = set(map(lambda word: word.lower().strip(), q1.split(" ")))
    w2 = set(map(lambda word: word.lower().strip(), q2.split(" ")))
    return len(w1 & w2)

def test_total_words(q1,q2):
    w1 = set(map(lambda word: word.lower().strip(), q1.split(" ")))
    w2 = set(map(lambda word: word.lower().strip(), q2.split(" ")))
    return (len(w1) + len(w2))

def test_fetch_token_features(q1,q2):

    SAFE_DIV = 0.0001

    STOP_WORDS = stopwords.words("english")

    token_features = [0.0]*8

    # Converting the Sentence into Tokens:
    q1_tokens = q1.split()
    q2_tokens = q2.split()

    if len(q1_tokens) == 0 or len(q2_tokens) == 0:
        return token_features

    # Get the non-stopwords in Questions
    q1_words = set([word for word in q1_tokens if word not in STOP_WORDS])
    q2_words = set([word for word in q2_tokens if word not in STOP_WORDS])

    #Get the stopwords in Questions
    q1_stops = set([word for word in q1_tokens if word in STOP_WORDS])
    q2_stops = set([word for word in q2_tokens if word in STOP_WORDS])

    # Get the common non-stopwords from Question pair
    common_word_count = len(q1_words.intersection(q2_words))

    # Get the common stopwords from Question pair
    common_stop_count = len(q1_stops.intersection(q2_stops))

    # Get the common Tokens from Question pair
    common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))


    token_features[0] = common_word_count / (min(len(q1_words), len(q2_words)) + SAFE_DIV)
    token_features[1] = common_word_count / (max(len(q1_words), len(q2_words)) + SAFE_DIV)
    token_features[2] = common_stop_count / (min(len(q1_stops), len(q2_stops)) + SAFE_DIV)
    token_features[3] = common_stop_count / (max(len(q1_stops), len(q2_stops)) + SAFE_DIV)
    token_features[4] = common_token_count / (min(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)
    token_features[5] = common_token_count / (max(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)

    # Last word of both question is same or not
    token_features[6] = int(q1_tokens[-1] == q2_tokens[-1])

    # First word of both question is same or not
    token_features[7] = int(q1_tokens[0] == q2_tokens[0])

    return token_features

def test_fetch_length_features(q1,q2):

    length_features = [0.0]*3

    # Converting the Sentence into Tokens:
    q1_tokens = q1.split()
    q2_tokens = q2.split()

    if len(q1_tokens) == 0 or len(q2_tokens) == 0:
        return length_features

    # Absolute length features
    length_features[0] = abs(len(q1_tokens) - len(q2_tokens))

    #Average Token Length of both Questions
    length_features[1] = (len(q1_tokens) + len(q2_tokens))/2

    strs = list(distance.lcsubstrings(q1, q2))
    length_features[2] = len(strs[0]) / (min(len(q1), len(q2)) + 1)

    return length_features

def test_fetch_fuzzy_features(q1,q2):

    fuzzy_features = [0.0]*4

    # fuzz_ratio
    fuzzy_features[0] = fuzz.QRatio(q1, q2)

    # fuzz_partial_ratio
    fuzzy_features[1] = fuzz.partial_ratio(q1, q2)

    # token_sort_ratio
    fuzzy_features[2] = fuzz.token_sort_ratio(q1, q2)

    # token_set_ratio
    fuzzy_features[3] = fuzz.token_set_ratio(q1, q2)

    return fuzzy_features

def query_point_creator(q1,q2):

    input_query = []

    # preprocess
    q1 = preprocess(q1)
    q2 = preprocess(q2)

    # fetch basic features
    input_query.append(len(q1))
    input_query.append(len(q2))

    input_query.append(len(q1.split(" ")))
    input_query.append(len(q2.split(" ")))

    input_query.append(test_common_words(q1,q2))
    input_query.append(test_total_words(q1,q2))
    input_query.append(round(test_common_words(q1,q2)/test_total_words(q1,q2),2))

    # fetch token features
    token_features = test_fetch_token_features(q1,q2)
    input_query.extend(token_features)

    # fetch length based features
    length_features = test_fetch_length_features(q1,q2)
    input_query.extend(length_features)

    # fetch fuzzy features
    fuzzy_features = test_fetch_fuzzy_features(q1,q2)
    input_query.extend(fuzzy_features)

    # w2v feature for q1
    q1_w2v = sentence_vector(q1.split(), w2v_model).reshape(1, -1)

    # w2v feature for q2
    q2_w2v = sentence_vector(q2.split(), w2v_model).reshape(1, -1)

    return np.hstack((np.array(input_query).reshape(1,22),q1_w2v,q2_w2v))

q1 = 'Where is the capital of India?'
q2 = 'What is the current capital of Pakistan?'
q3 = 'Which city serves as the capital of India?'
q4 = 'What is the business capital of India?'

rf.predict(query_point_creator(q1,q4))

import pickle

# save trained RandomForest
pickle.dump(rf, open("model.pkl", "wb"))

# save trained Word2Vec model
pickle.dump(w2v_model, open("w2v_model.pkl", "wb"))

os.listdir()

